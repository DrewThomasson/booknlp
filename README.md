# BookNLP

A natural language processing pipeline for analyzing works of fiction, including entity detection, quotation attribution, and character relationship analysis.

## Prerequisites

- Python 3.9 or higher
- pip (Python package installer)
- Virtual environment (recommended)

## Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/booknlp.git
cd booknlp
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows, use: venv\Scripts\activate
```

3. Install the required packages:
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

4. Install the spaCy model:
```bash
python -m spacy download en_core_web_sm
```

## Usage

1. Make sure your virtual environment is activated:
```bash
source venv/bin/activate  # On Windows, use: venv\Scripts\activate
```

2. Run BookNLP on a text file:
```bash
./run_booknlp.py input_file.txt --output-dir output/directory
```

### Command Line Arguments

- `input_file`: The text file to process (required)
- `--output-dir`: Directory where output files will be saved (default: 'output')
- `--model`: Model size to use - 'big' or 'small' (default: 'small')

### Output Files

The pipeline generates several output files in the specified output directory:

1. `{book_id}.tokens`: Word-level information including:
   - Paragraph and sentence IDs
   - Word forms and lemmas
   - Part-of-speech tags
   - Dependency relations
   - Event annotations

2. `{book_id}.entities`: Named entity information including:
   - Entity types
   - Coreference IDs
   - Text spans

3. `{book_id}.quotes`: Quotation information including:
   - Quoted text
   - Speaker attribution
   - Coreference information

4. `{book_id}.supersense`: Semantic categories for words:
   - Verb categories
   - Noun categories

5. `{book_id}.event`: Event annotations including:
   - Event types
   - Participants
   - Temporal information

6. `{book_id}.book`: JSON file containing:
   - Character information
   - Relationships
   - Actions
   - Attributes

7. `{book_id}.book.html`: Interactive HTML visualization of the text with:
   - Entity annotations
   - Character relationships
   - Interactive features

## Example

```bash
# Process a text file named "emma.txt" with maximum accuracy
./run_booknlp.py emma.txt --output-dir output/emma --model big
```

## Improving Results

For the best possible results, consider the following:

1. **Use the 'big' model**:
   ```bash
   ./run_booknlp.py input.txt --output-dir output --model big
   ```
   The big model provides significantly higher accuracy for all tasks (2-4% improvement).

2. **Enable full coreference resolution**:
   By default, the script now uses `"pronominalCorefOnly": False` which enables more comprehensive entity linking.

3. **Selective pipeline components**:
   If you only need specific analysis, you can run selected components:
   ```bash
   ./run_booknlp.py input.txt --output-dir output --pipeline entity,quote,coref
   ```

## Analysis and Visualization

The repository includes a script to analyze and visualize BookNLP outputs:

```bash
# Install additional requirements
pip install -r analysis_requirements.txt

# Run analysis on processed files
./analyze_output.py output/emma --output-dir analysis/emma
```

The analysis script generates:

1. **Character information**: CSV file with character stats
2. **Character network**: Network visualization of character relationships
3. **Quote distribution**: Bar chart of quotes by character

## Troubleshooting

If you encounter any issues:

1. Make sure your virtual environment is activated
2. Verify that all dependencies are installed correctly
3. Check that the spaCy model is installed
4. Ensure you have sufficient disk space for the output files
5. Make sure the input file exists and is readable

## License

MIT

# BookNLP Output Guide

This guide explains the output files generated by BookNLP and provides tools to help you understand and visualize them.

## Output Files

BookNLP generates several output files for each book processed:

| File Extension | Description |
|----------------|-------------|
| `.entities` | Character and entity references with coreference resolution |
| `.quotes` | Direct speech with speaker attribution |
| `.tokens` | Tokenized text with POS tags, lemmas, and other annotations |
| `.supersense` | Semantic categorization of entities |
| `.book.html` | HTML visualization of the book with annotations |
| `.book` | The book text in a single line |

## Understanding the Files

### Entities File (`.entities`)

This file tracks characters and other entities throughout the text:

```
COREF  start_token  end_token  prop  cat  text
13     8           16         NOM   PER  a single man in possession of a good fortune
```

- `COREF`: Entity ID (used to track the same entity across mentions)
- `start_token/end_token`: Token position in the document
- `prop`: Type of reference (PROP for proper nouns, NOM for nominal, PRON for pronouns)
- `cat`: Entity category (PER for person, LOC for location, GPE for geo-political entity, FAC for facility)
- `text`: The actual text referring to the entity

### Quotes File (`.quotes`)

This file contains direct speech with speaker attribution:

```
quote_start  quote_end  mention_start  mention_end  mention_phrase  char_id  quote
76          82        84              85           his lady       13      " My dear Mr. Bennet , "
```

- `quote_start/quote_end`: Token positions of the quote
- `mention_start/mention_end`: Token positions of the speaker mention
- `mention_phrase`: The phrase referring to the speaker
- `char_id`: Character ID (matches the COREF ID in the entities file)
- `quote`: The actual quote text

### Tokens File (`.tokens`)

This file contains detailed information about each token:

```
paragraph_ID  sentence_ID  token_ID_within_sentence  token_ID_within_document  word  lemma  byte_onset  byte_offset  POS_tag  fine_POS_tag  dependency_relation  syntactic_head_ID  event
```

- `paragraph_ID/sentence_ID`: Position identifiers
- `token_ID_within_sentence/token_ID_within_document`: Token position
- `word/lemma`: Token text and its lemmatized form
- `byte_onset/byte_offset`: Character positions in the original text
- `POS_tag/fine_POS_tag`: Part-of-speech tags
- `dependency_relation/syntactic_head_ID`: Dependency parsing information
- `event`: Marks if the token is part of an event (O for not an event, EVENT for events)

